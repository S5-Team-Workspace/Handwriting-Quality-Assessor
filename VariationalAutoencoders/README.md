# Variational Autoencoders (VAE) Implementation

A comprehensive implementation of Variational Autoencoders using PyTorch, featuring detailed training, visualization, and analysis capabilities.

## ğŸ¯ Overview

This repository contains a complete implementation of Variational Autoencoders (VAEs) trained on the MNIST dataset. The implementation includes:

- **Complete VAE Architecture**: Encoder, decoder, and reparameterization trick
- **Comprehensive Training System**: With logging, model checkpointing, and metrics tracking
- **Rich Visualizations**: Latent space plots, reconstructions, interpolations, and generated samples
- **Detailed Documentation**: Both in code and this README

## ğŸ—ï¸ Architecture

The VAE implementation features:

- **Input Dimension**: 784 (28Ã—28 MNIST images)
- **Latent Dimension**: 2 (for easy visualization)
- **Hidden Dimensions**: 256 â†’ 128 in encoder, 128 â†’ 256 in decoder
- **Regularization**: Batch normalization and dropout layers
- **Loss Function**: Binary cross-entropy reconstruction loss + KL divergence

### Model Components

1. **Encoder**: Maps input images to latent distribution parameters (Î¼, ÏƒÂ²)
2. **Reparameterization**: Samples latent variables using the reparameterization trick
3. **Decoder**: Reconstructs images from latent variables

## ğŸ“Š Results

The trained model achieves:

- **Reconstruction Quality**: Clear digit reconstructions
- **Latent Space Organization**: Well-separated digit clusters in 2D latent space
- **Generation Capability**: Smooth interpolations between different digits
- **Training Stability**: Balanced reconstruction and KL losses

### Generated Samples

![Generated Samples](vae_logs_latent2_beta1.0/generated_samples.png)

### Latent Space Visualization

![Latent Space](vae_logs_latent2_beta1.0/latent_space_visualization.png)

### Reconstruction Comparison

![Reconstructions](vae_logs_latent2_beta1.0/reconstruction_comparison.png)

### Training Curves

![Training Metrics](vae_logs_latent2_beta1.0/comprehensive_training_curves.png)

## ğŸš€ Quick Start

### Prerequisites

```bash
pip install torch torchvision matplotlib pandas numpy seaborn
```

### Running the Code

1. Clone this repository:
```bash
git clone https://github.com/GruheshKurra/VariationalAutoencoders.git
cd VariationalAutoencoders
```

2. Open and run the Jupyter notebook:
```bash
jupyter notebook Untitled.ipynb
```

The notebook will automatically:
- Download and preprocess the MNIST dataset
- Train the VAE model
- Generate visualizations and save results
- Create comprehensive training logs

## ğŸ“ Repository Structure

```
â”œâ”€â”€ Untitled.ipynb                 # Main implementation notebook
â”œâ”€â”€ vae_logs_latent2_beta1.0/      # Training results and visualizations
â”‚   â”œâ”€â”€ best_vae_model.pth         # Trained model weights
â”‚   â”œâ”€â”€ training_metrics.csv       # Training metrics
â”‚   â”œâ”€â”€ generated_samples.png      # Generated digit samples
â”‚   â”œâ”€â”€ latent_space_visualization.png  # 2D latent space plot
â”‚   â”œâ”€â”€ reconstruction_comparison.png   # Original vs reconstructed
â”‚   â”œâ”€â”€ latent_interpolation.png   # Interpolation between digits
â”‚   â””â”€â”€ comprehensive_training_curves.png  # Training curves
â”œâ”€â”€ pytorch_vae_logs/              # Additional training logs
â”œâ”€â”€ data/                          # MNIST dataset (auto-downloaded)
â”œâ”€â”€ grok.md                        # Detailed VAE explanation
â””â”€â”€ README.md                      # This file
```

## ğŸ§  Key Features

### 1. Comprehensive VAE Implementation
- Clean, well-documented PyTorch implementation
- Modular design with separate classes for model, trainer, logger, and visualizer
- Support for different latent dimensions and beta values

### 2. Advanced Training System
- Automatic model checkpointing
- Comprehensive metrics logging
- Learning rate scheduling
- Early stopping capabilities

### 3. Rich Visualizations
- **Latent Space Plots**: Visualize learned representations
- **Reconstructions**: Compare original and reconstructed images
- **Generated Samples**: New digits sampled from the latent space
- **Interpolations**: Smooth transitions between different digits
- **Training Curves**: Monitor loss components over time

### 4. Detailed Analysis
- Training metrics tracking (reconstruction loss, KL loss, total loss)
- Model performance evaluation
- Latent space analysis and interpretation

## ğŸ”§ Customization

The implementation supports easy customization:

```python
# Different latent dimensions
latent_dim = 10  # Higher dimensional latent space

# Different beta values (Î²-VAE)
beta = 0.5  # Lower beta for better reconstructions
beta = 4.0  # Higher beta for better disentanglement

# Different architectures
hidden_dim = 512  # Larger hidden layers
```

## ğŸ“š Educational Value

This repository is designed for learning and includes:

- **Detailed Comments**: Every line of code is explained
- **Mathematical Background**: Complete loss function derivations
- **Visualization Examples**: Understanding what VAEs learn
- **Training Analysis**: How to monitor and improve VAE training

## ğŸ¯ Use Cases

This VAE implementation can be adapted for:

- **Image Generation**: Generate new images similar to training data
- **Data Compression**: Efficient representation learning
- **Anomaly Detection**: Identify outliers using reconstruction error
- **Data Augmentation**: Generate synthetic training samples
- **Representation Learning**: Learn meaningful features

## ğŸ“ˆ Performance

Training on MNIST (60,000 images):
- **Training Time**: ~10 minutes on CPU, ~2 minutes on GPU
- **Model Size**: ~1.8MB
- **Parameters**: ~400K trainable parameters

## ğŸ¤ Contributing

Feel free to contribute by:
- Adding new datasets
- Implementing different VAE variants (Î²-VAE, WAE, etc.)
- Improving visualizations
- Adding more comprehensive documentation

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ“¬ Contact

For questions or suggestions, please open an issue in this repository.

---

**Happy Learning! ğŸ“**

*This implementation provides a solid foundation for understanding and experimenting with Variational Autoencoders.*
